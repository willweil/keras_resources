{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Quick Reference\n",
    "\n",
    "- Official document: https://keras.io/layers/normalization/\n",
    "- Reference 1: https://keras.io/getting-started/sequential-model-guide/\n",
    "- Reference 2: https://machinelearningmastery.com/introduction-python-deep-learning-library-keras/\n",
    "\n",
    "**Steps to build neural network with Keras**:\n",
    "1. Construct Model\n",
    "2. Compile Model\n",
    "3. Fit Model\n",
    "4. Evaluate Model\n",
    "5. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import keras\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)  # set random seed for repeatibility\n",
    "print(K._BACKEND) # check backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pima indians dataset\n",
    "dataset = np.loadtxt(\"data/pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (768, 8)\n",
      "Y.shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Construct the neural network\n",
    "*Need specify input shape to the first layer*:\n",
    "- **input_shape** is a shape tuple (a tuple of integers or None entries, where None indicates that any positive integer may be expected). In input_shape, the batch dimension is not included.\n",
    "- Some 2D layers, such as Dense, support the specification of their input shape via the argument **input_dim**, and some 3D temporal layers support the arguments input_dim and **input_length**.\n",
    "- If you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks), you can pass a **batch_size** argument to a layer. If you pass both batch_size=32 and input_shape=(6, 8) to a layer, it will then expect every batch of inputs to have the batch shape (32, 6, 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=12, input_dim=8, activation='relu'))  # 8 features\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to construct a model\n",
    "model = Sequential([\n",
    "    Dense(12, input_shape=(8,)),\n",
    "    Activation('relu'),\n",
    "    Dense(8),\n",
    "    Activation('relu'),\n",
    "    Dense(1),\n",
    "    Activation('sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model\n",
    "*Compile a model means to configure the learning process. It has three arguments*:\n",
    "\n",
    "A **loss function**. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function. \n",
    "- binary_crossentropy\n",
    "- categorical_crossentropy \n",
    "- sparse_categorical_crossentropy \n",
    "- poisson \n",
    "- hinge \n",
    "- categorical_hinge \n",
    "- mean_absolute_error \n",
    "- mean_squared_error\n",
    "\n",
    "An **optimizer**. This could be the string identifier of an existing optimizer (such as rmsprop or adagrad), or an instance of the Optimizer class.\n",
    "- SGD \n",
    "- RMSprop \n",
    "- Adam \n",
    "- Nadam \n",
    "- Adamax\n",
    "\n",
    "A list of **metrics**. For any classification problem you will want to set this to metrics=['accuracy']. A metric could be the string identifier of an existing metric or a custom metric function. A metric function is similar to a loss function, except that the results from evaluating a metric are not used when training the model. You may use any of the loss functions as a metric function.\n",
    "- For any classification problem, set metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model\n",
    "- Input data and labels need to be Numpy arrays --just like in the Scikit-Learn API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.6788 - acc: 0.6615\n",
      "Epoch 2/20\n",
      "768/768 [==============================] - 0s 99us/step - loss: 0.6764 - acc: 0.6602\n",
      "Epoch 3/20\n",
      "768/768 [==============================] - 0s 105us/step - loss: 0.6459 - acc: 0.6719\n",
      "Epoch 4/20\n",
      "768/768 [==============================] - 0s 96us/step - loss: 0.6414 - acc: 0.6771\n",
      "Epoch 5/20\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.6336 - acc: 0.6784\n",
      "Epoch 6/20\n",
      "768/768 [==============================] - 0s 107us/step - loss: 0.6390 - acc: 0.6797\n",
      "Epoch 7/20\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.6452 - acc: 0.6784\n",
      "Epoch 8/20\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.6481 - acc: 0.6693\n",
      "Epoch 9/20\n",
      "768/768 [==============================] - 0s 99us/step - loss: 0.6159 - acc: 0.6615\n",
      "Epoch 10/20\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.6155 - acc: 0.6888\n",
      "Epoch 11/20\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.6203 - acc: 0.6719\n",
      "Epoch 12/20\n",
      "768/768 [==============================] - 0s 70us/step - loss: 0.6162 - acc: 0.6654\n",
      "Epoch 13/20\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.6198 - acc: 0.674 - 0s 108us/step - loss: 0.6019 - acc: 0.6797\n",
      "Epoch 14/20\n",
      "768/768 [==============================] - 0s 101us/step - loss: 0.6141 - acc: 0.6979\n",
      "Epoch 15/20\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.6003 - acc: 0.6914\n",
      "Epoch 16/20\n",
      "768/768 [==============================] - 0s 98us/step - loss: 0.5953 - acc: 0.6901\n",
      "Epoch 17/20\n",
      "768/768 [==============================] - 0s 99us/step - loss: 0.6156 - acc: 0.6602\n",
      "Epoch 18/20\n",
      "768/768 [==============================] - 0s 69us/step - loss: 0.6195 - acc: 0.6706\n",
      "Epoch 19/20\n",
      "768/768 [==============================] - 0s 98us/step - loss: 0.5887 - acc: 0.6875\n",
      "Epoch 20/20\n",
      "768/768 [==============================] - 0s 107us/step - loss: 0.5973 - acc: 0.6914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e614756a20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=20, batch_size=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 13us/step\n",
      "acc 70.1822916667\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X, Y, batch_size=128)\n",
    "print(model.metrics_names[1], scores[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X)\n",
    "predicted_class = [1 if x >= 0.5 else 0 for x in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot model structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 112.00 264.00\" width=\"112pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-260 108,-260 108,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2087805548248 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2087805548248</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 104,-182.5 104,-146.5 0,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52\" y=\"-160.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 2087805548640 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2087805548640</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 104,-109.5 104,-73.5 0,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52\" y=\"-87.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 2087805548248&#45;&gt;2087805548640 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2087805548248-&gt;2087805548640</title>\n",
       "<path d=\"M52,-146.313C52,-138.289 52,-128.547 52,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"55.5001,-119.529 52,-109.529 48.5001,-119.529 55.5001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2087805549480 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2087805549480</title>\n",
       "<polygon fill=\"none\" points=\"0,-0.5 0,-36.5 104,-36.5 104,-0.5 0,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52\" y=\"-14.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 2087805548640&#45;&gt;2087805549480 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2087805548640-&gt;2087805549480</title>\n",
       "<path d=\"M52,-73.3129C52,-65.2895 52,-55.5475 52,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"55.5001,-46.5288 52,-36.5288 48.5001,-46.5289 55.5001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2087805549032 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2087805549032</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 104,-255.5 104,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52\" y=\"-233.8\">2087805549032</text>\n",
       "</g>\n",
       "<!-- 2087805549032&#45;&gt;2087805548248 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2087805549032-&gt;2087805548248</title>\n",
       "<path d=\"M52,-219.313C52,-211.289 52,-201.547 52,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"55.5001,-192.529 52,-182.529 48.5001,-192.529 55.5001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='my_model.png')\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Keras models to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ResNet50 as an example model\n",
    "from keras.applications import resnet50\n",
    "my_model = resnet50.ResNet50(include_top=True, weights='imagenet')\n",
    "my_model.load_weights('resnet50_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "my_model.fit(X, Y, epochs=30, batch_size=12, verbose=0)\n",
    "scores = my_model.evaluate(X, Y, verbose=0)\n",
    "print(my_model.metrics_names[1], \": \\n\", scores[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Save the **entire** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json, load_model\n",
    "import json\n",
    "\n",
    "# save entire model\n",
    "my_model.save('my_model.h5')\n",
    "\n",
    "# load model\n",
    "loaded_model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Save **Weights + Architecture** to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save Weights + Architecture\n",
    "my_model.save_weights('my_model_weights.h5')\n",
    "with open('my_model_architecture.json', 'w') as f:\n",
    "    f.write(my_model.to_json())\n",
    "\n",
    "# load Weights + Architecture\n",
    "with open('my_model_architecture.json', 'r') as f:\n",
    "    loaded_model = model_from_json(f.read())\n",
    "loaded_model.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 3: Save **Weights + Architecture** to YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save Weights + Architecture\n",
    "my_model.save_weights(\"my_model_weights.h5\")\n",
    "with open(\"my_model_architecture.yaml\", \"w\") as f:\n",
    "    f.write(my_model.to_yaml())\n",
    "\n",
    "# load Weights + Architecture\n",
    "with open('my_model_architecture.yaml', 'r') as f:\n",
    "    loaded_model = model_from_yaml(f.read())\n",
    "loaded_model.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate loaded model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "scores = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(loaded_model.metrics_names[1], ': \\n', scores[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a single-input model with 2 classes (binary classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a single-input model with 10 classes (categorical classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilayer Perceptron (MLP) for multi-class softmax classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP for binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = np.random.randint(2, size=(1000, 1))\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence classification with LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "max_features = 1024\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, output_dim=256))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=16, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence classification with 1D convolutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "\n",
    "seq_length = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 3, activation='relu', input_shape=(seq_length, 100)))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=16, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked LSTM for sequence classification\n",
    "In this model, we stack 3 LSTM layers on top of each other, making the model capable of learning higher-level temporal representations.\n",
    "\n",
    "The first two LSTMs return their full output sequences, but the last one only returns the last step in its output sequence, thus dropping the temporal dimension (i.e. converting the input sequence into a single vector).\n",
    "<img src=\"images/regular_stacked_lstm.png\" style=\"width:250px;height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 16\n",
    "timesteps = 8\n",
    "num_classes = 10\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32))  # return a single vector of dimension 32\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy training data\n",
    "x_train = np.random.random((1000, timesteps, data_dim))\n",
    "y_train = np.random.random((1000, num_classes))\n",
    "\n",
    "# Generate dummy validation data\n",
    "x_val = np.random.random((100, timesteps, data_dim))\n",
    "y_val = np.random.random((100, num_classes))\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64, epochs=5,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same stacked LSTM model, rendered \"stateful\"\n",
    "A stateful recurrent model is one for which the internal states (memories) obtained after processing a batch of samples are reused as initial states for the samples of the next batch. This allows to process longer sequences while keeping computational complexity manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 16\n",
    "timesteps = 8\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Expected input batch shape: (batch_size, timesteps, data_dim)\n",
    "# Note that we have to provide the full batch_input_shape since the network is stateful.\n",
    "# the sample of index i in batch k is the follow-up for the sample i in batch k-1.\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True,\n",
    "               batch_input_shape=(batch_size, timesteps, data_dim)))\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True))\n",
    "model.add(LSTM(32, stateful=True))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy training data\n",
    "x_train = np.random.random((batch_size * 10, timesteps, data_dim))\n",
    "y_train = np.random.random((batch_size * 10, num_classes))\n",
    "\n",
    "# Generate dummy validation data\n",
    "x_val = np.random.random((batch_size * 3, timesteps, data_dim))\n",
    "y_val = np.random.random((batch_size * 3, num_classes))\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size, epochs=5, shuffle=False,\n",
    "          validation_data=(x_val, y_val))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
